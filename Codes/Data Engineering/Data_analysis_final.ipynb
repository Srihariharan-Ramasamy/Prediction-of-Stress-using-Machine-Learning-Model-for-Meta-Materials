{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe93026d-f96b-472e-960f-7863d7ea58dd",
   "metadata": {},
   "source": [
    "DATA ANALYSIS CODE\n",
    "1) Initial data study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87adb5f5-8f0b-4024-9209-ec58f523e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading the dataset\n",
    "CSV_PATH = \"sff_optimal_1.csv\"   \n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "bucket_col = df.columns[-1]\n",
    "buckets = df[bucket_col].astype(int)\n",
    "last_bucket = int(buckets.iloc[-1])\n",
    "counts = buckets.value_counts().reindex([0,1,2,3], fill_value=0)\n",
    "total_n = len(buckets)\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.figure()\n",
    "bars = plt.bar([0,1,2,3], counts.values)\n",
    "plt.xticks([0,1,2,3], [\"0\",\"1\",\"2\",\"3\"])\n",
    "plt.xlabel(\"Bucket\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(f\"Bucket Frequency (N={total_n})\")\n",
    "\n",
    "\n",
    "for rect, cnt in zip(bars, counts.values):\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2.0, height, str(int(cnt)),\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af99b5c-03e9-4085-806b-60f3b1332498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading the dataset and defining output file\n",
    "# Mean usage < 1%, nonzero in <5% of samples\n",
    "CSV_PATH = \"sff_optimal_1.csv\"               \n",
    "SFF_PREFIX = \"sff_\"                 \n",
    "ZERO_TOL = 1e-12                     \n",
    "RARE_NONZERO_RATE = 0.05             \n",
    "RARE_MEAN = 0.01                    \n",
    "EXPORT_CSV = \"sff_usage_metrics_final.csv\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Safety check if prefix not found\n",
    "sff_cols = [c for c in df.columns if c.startswith(SFF_PREFIX)]\n",
    "if not sff_cols:\n",
    "    raise ValueError(f\"No columns starting with '{SFF_PREFIX}' were found.\")\n",
    "\n",
    "SFF = df[sff_cols].copy()\n",
    "\n",
    "# Calculation of metrics\n",
    "n = len(SFF)\n",
    "nonzero_rate = (SFF > ZERO_TOL).sum(axis=0) / n\n",
    "mean_usage   = SFF.mean(axis=0)\n",
    "median_usage = SFF.median(axis=0)\n",
    "p95_usage    = SFF.quantile(0.95, axis=0)\n",
    "cv_usage     = (SFF.std(axis=0) / (mean_usage.replace(0, np.nan))).fillna(np.nan)  # coefficient of variation\n",
    "zero_share   = (SFF <= ZERO_TOL).sum(axis=0) / n\n",
    "\n",
    "metrics = pd.DataFrame({\n",
    "    \"sff\": sff_cols,\n",
    "    \"nonzero_rate\": nonzero_rate.values,\n",
    "    \"mean\": mean_usage.values,\n",
    "    \"median\": median_usage.values,\n",
    "    \"p95\": p95_usage.values,\n",
    "    \"cv\": cv_usage.values,\n",
    "    \"zero_share\": zero_share.values,\n",
    "}).sort_values([\"nonzero_rate\", \"mean\"], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "# Flag “underused” by thresholds\n",
    "metrics[\"rare_flag\"] = (\n",
    "    (metrics[\"nonzero_rate\"] < RARE_NONZERO_RATE) | (metrics[\"mean\"] < RARE_MEAN)\n",
    ")\n",
    "\n",
    "print(\"=== Underused SFFs (by thresholds) ===\")\n",
    "print(metrics.loc[metrics[\"rare_flag\"], [\"sff\", \"nonzero_rate\", \"mean\"]])\n",
    "\n",
    "# Exporting\n",
    "metrics.to_csv(EXPORT_CSV, index=False)\n",
    "print(f\"\\nSaved metrics table to: {EXPORT_CSV}\")\n",
    "\n",
    "# Plotting\n",
    "metrics_nr = metrics.sort_values(\"nonzero_rate\", ascending=True)\n",
    "plt.figure()\n",
    "ypos = np.arange(len(metrics_nr))\n",
    "bars = plt.barh(ypos, metrics_nr[\"nonzero_rate\"].values)\n",
    "plt.yticks(ypos, metrics_nr[\"sff\"].values)\n",
    "plt.xlabel(\"Nonzero rate\")\n",
    "plt.title(\"SFF Nonzero Rate (lower = used less often)\")\n",
    "for i, (rate, rare) in enumerate(zip(metrics_nr[\"nonzero_rate\"].values, metrics_nr[\"rare_flag\"].values)):\n",
    "    if rare:\n",
    "        plt.text(rate, i, \"  ⭑\", va='center')  # star mark\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "metrics_mean = metrics.sort_values(\"mean\", ascending=True)\n",
    "plt.figure()\n",
    "ypos = np.arange(len(metrics_mean))\n",
    "bars = plt.barh(ypos, metrics_mean[\"mean\"].values)\n",
    "plt.yticks(ypos, metrics_mean[\"sff\"].values)\n",
    "plt.xlabel(\"Mean usage (probability)\")\n",
    "plt.title(\"SFF Mean Usage (lower = used less)\")\n",
    "for i, (m, rare) in enumerate(zip(metrics_mean[\"mean\"].values, metrics_mean[\"rare_flag\"].values)):\n",
    "    if rare:\n",
    "        plt.text(m, i, \"  ⭑\", va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "K = 10  \n",
    "bottom_cols = metrics_nr[\"sff\"].head(K).tolist()\n",
    "plt.figure()\n",
    "plt.boxplot([SFF[c].values for c in bottom_cols], vert=False, labels=bottom_cols, showfliers=False)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.title(f\"SFF Distributions — Bottom {K} by Nonzero Rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550111a3-e576-4615-8aa5-7dd2175dda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading the file\n",
    "CSV_PATH = \"sff_optimal_1.csv\"\n",
    "FEATURE_COLS = slice(0, 25)   \n",
    "GRID_SHAPE = (5, 5)           \n",
    "GRID_ORDER = \"row-major\"      \n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Metrics\n",
    "X = df.iloc[:, FEATURE_COLS].astype(int).to_numpy()   # shape: (n_samples, 25)\n",
    "n_samples = X.shape[0]\n",
    "\n",
    "# counts and rates per position\n",
    "counts = X.sum(axis=0)                # length 25\n",
    "rates  = counts / n_samples\n",
    "\n",
    "# reshape to grid\n",
    "order = 'C' if GRID_ORDER == \"row-major\" else 'F'\n",
    "grid_counts = counts.reshape(GRID_SHAPE, order=order)\n",
    "grid_rates  = rates.reshape(GRID_SHAPE,  order=order)\n",
    "\n",
    "print(f\"Total samples: {n_samples}\")\n",
    "print(\"Counts per feature (0..24):\")\n",
    "print(counts)\n",
    "\n",
    "# Plotting heatmap (occupancy rate)\n",
    "plt.figure()\n",
    "im = plt.imshow(grid_rates, vmin=0, vmax=1)\n",
    "plt.title(\"Position occupancy rate (P(material=1))\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.colorbar(im, label=\"rate\")\n",
    "h, w = GRID_SHAPE\n",
    "for i in range(h):\n",
    "    for j in range(w):\n",
    "        plt.text(j, i, f\"{int(grid_counts[i,j])}\\n({grid_rates[i,j]:.2f})\",\n",
    "                 ha='center', va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting contour \n",
    "# build coordinate grid for contouring\n",
    "yy, xx = np.mgrid[0:h, 0:w]\n",
    "plt.figure()\n",
    "levels = np.linspace(grid_rates.min(), grid_rates.max(), 7)\n",
    "cs = plt.contourf(xx, yy, grid_rates, levels=levels)\n",
    "plt.title(\"5×5 contour of occupancy rate\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.colorbar(cs, label=\"rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a0d91-3ab2-4ce7-a3bd-898fb04bec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset and rearranging into the grid for heatmaps\n",
    "CSV_PATH = \"sff_optimal_1.csv\"         \n",
    "FEATURE_COLS = slice(0, 25)   \n",
    "BUCKET_IS_LAST_COL = True\n",
    "BUCKET_COL = None            \n",
    "GRID_SHAPE = (5, 5)\n",
    "GRID_ORDER = \"row-major\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Safety check\n",
    "if BUCKET_COL is None and BUCKET_IS_LAST_COL:\n",
    "    BUCKET_COL = df.columns[-1]\n",
    "\n",
    "# Feature matrix\n",
    "X = df.iloc[:, FEATURE_COLS].astype(int).to_numpy()\n",
    "order = 'C' if GRID_ORDER == \"row-major\" else 'F'\n",
    "\n",
    "# Plotting heatmap per class\n",
    "classes = [0, 1, 2, 3]\n",
    "for c in classes:\n",
    "    subset = df[df[BUCKET_COL] == c]\n",
    "    if len(subset) == 0:\n",
    "        print(f\"Class {c}: no samples, skipping.\")\n",
    "        continue\n",
    "\n",
    "    rates = subset.iloc[:, FEATURE_COLS].mean().to_numpy()           \n",
    "    grid = rates.reshape(GRID_SHAPE, order=order)\n",
    "\n",
    "    counts = subset.iloc[:, FEATURE_COLS].sum().to_numpy()           \n",
    "    grid_counts = counts.reshape(GRID_SHAPE, order=order)\n",
    "\n",
    "    plt.figure()\n",
    "    im = plt.imshow(grid, vmin=0, vmax=1)\n",
    "    plt.title(f\"Occupancy heatmap | class = {c} (N={len(subset)})\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.colorbar(im, label=\"P(material=1)\")\n",
    "\n",
    "    # annotate: count and rate\n",
    "    h, w = GRID_SHAPE\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            plt.text(j, i, f\"{int(grid_counts[i,j])}\\n({grid[i,j]:.2f})\",\n",
    "                     ha='center', va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5460e-aef4-4496-9442-c321495ae961",
   "metadata": {},
   "source": [
    "2) Feature engineering to check which features are most influential to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f5853-896c-46ee-9e0d-14081117548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.stats import chi2_contingency\n",
    "# loading the dataset\n",
    "df = pd.read_csv(\"sff_optimal_1.csv\")\n",
    "bits = df.iloc[:, 0:25].astype(int)\n",
    "sff  = df.iloc[:, 25:25+26].astype(float)\n",
    "y    = df.iloc[:, -1].astype(int)  # bucket 0..3\n",
    "n    = len(df)\n",
    "\n",
    "# Calculating the mutual information\n",
    "X_bits = bits.values\n",
    "X_sff  = sff.values\n",
    "mi_bits = mutual_info_classif(X_bits, y, discrete_features=True, random_state=0)\n",
    "mi_sff  = mutual_info_classif(X_sff,  y, discrete_features=False, random_state=0)\n",
    "mi_bits_s = pd.Series(mi_bits, index=bits.columns).sort_values(ascending=False)\n",
    "mi_sff_s  = pd.Series(mi_sff,  index=sff.columns).sort_values(ascending=False)\n",
    "\n",
    "# χ² + Cramér’s V for bits\n",
    "classes_present = sorted(pd.unique(y))  \n",
    "\n",
    "def cramers_v_safe(table_values):\n",
    "    \"\"\"\n",
    "    table_values: numpy array (r x c) of nonnegative counts.\n",
    "    Returns (V, p_value) or (np.nan, np.nan) if undefined.\n",
    "    \"\"\"\n",
    "    n = table_values.sum()\n",
    "    if n == 0:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # if any row or column sum is zero\n",
    "    if (table_values.sum(axis=0) == 0).any() or (table_values.sum(axis=1) == 0).any():\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # degrees of freedom must be > 0\n",
    "    r, c = table_values.shape\n",
    "    if r < 2 or c < 2:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    chi2, p, dof, _ = chi2_contingency(table_values, correction=False)\n",
    "    denom = n * (min(r - 1, c - 1))\n",
    "    if denom <= 0:\n",
    "        return np.nan, p\n",
    "    V = np.sqrt(chi2 / denom)\n",
    "    return V, p\n",
    "\n",
    "rows = []\n",
    "for col in bits.columns:\n",
    "    # build contingency only over non zero classes\n",
    "    tab = pd.crosstab(bits[col], y)\n",
    "    tab = tab.reindex(index=[0, 1], columns=classes_present, fill_value=0)\n",
    "\n",
    "    V, p = cramers_v_safe(tab.values)\n",
    "    rows.append({\n",
    "        \"feature\": col,\n",
    "        \"cramers_v\": V,\n",
    "        \"p_value\": p,\n",
    "        \"count_ones\": int(bits[col].sum()),\n",
    "        \"count_zeros\": int((bits[col] == 0).sum())\n",
    "    })\n",
    "\n",
    "cv_bits = pd.DataFrame(rows).sort_values(\"cramers_v\", ascending=False).reset_index(drop=True)\n",
    "print(cv_bits.head(10))\n",
    "\n",
    "# Conditional probabilities for bits\n",
    "def cond_probs_bit(col):\n",
    "    sub1 = df[bits[col]==1][y.name].value_counts().reindex([0,1,2,3], fill_value=0)\n",
    "    sub0 = df[bits[col]==0][y.name].value_counts().reindex([0,1,2,3], fill_value=0)\n",
    "    p1 = (sub1 / max(1, sub1.sum())).values\n",
    "    p0 = (sub0 / max(1, sub0.sum())).values\n",
    "    return p1, p0\n",
    "\n",
    "\n",
    "top_bits = mi_bits_s.head(10).index\n",
    "diff_mat = []\n",
    "for col in top_bits:\n",
    "    p1, p0 = cond_probs_bit(col)\n",
    "    diff_mat.append(p1 - p0)\n",
    "diff_mat = np.vstack(diff_mat)  # 10x4\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(diff_mat, vmin=-1, vmax=1)\n",
    "plt.yticks(range(len(top_bits)), top_bits)\n",
    "plt.xticks(range(4), [0,1,2,3])\n",
    "plt.title(\"ΔP(class | bit=1) vs bit=0 (top-10 MI bits)\")\n",
    "plt.colorbar(label=\"Probability shift\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Class-conditional means for SFFs \n",
    "sff_means = sff.assign(cls=y).groupby(\"cls\").mean().T  # rows=SFFs, cols=classes\n",
    "# Plotting the top-10 class-discriminative SFFs by std across classes\n",
    "std_across = sff_means.std(axis=1).sort_values(ascending=False)\n",
    "pick = std_across.head(10).index\n",
    "plt.figure()\n",
    "plt.imshow(sff_means.loc[pick].values, aspect='auto')\n",
    "plt.yticks(range(len(pick)), pick); plt.xticks(range(4), [0,1,2,3])\n",
    "plt.title(\"E[SFF | class] for top-10 discriminative SFFs\")\n",
    "plt.colorbar(label=\"mean value\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Volume fraction checks\n",
    "vf = bits.sum(axis=1) / 25.0\n",
    "plt.figure(); plt.hist(vf, bins=20); plt.title(\"Volume fraction (overall)\"); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for c in [0,1,2,3]:\n",
    "    plt.hist(vf[y==c], bins=20, alpha=0.6)\n",
    "plt.title(\"Volume fraction by class\"); plt.legend([0,1,2,3]); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ecc94b-77e6-4559-a487-2715b731908b",
   "metadata": {},
   "source": [
    "3) Correlation matrix of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188294df-b594-410c-9ce5-dbaea321723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "# Load the dataset and running some checks \n",
    "CSV_PATH = \"sff_optimal_1.csv\"     \n",
    "BITS = slice(0, 25)                        \n",
    "SFF  = slice(25, 51)                       \n",
    "ANNOTATE = False                          \n",
    "ANNOTATE_THRESH = 0.5                      \n",
    "FIGSIZE = (12, 10)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "bits = df.iloc[:, BITS].astype(float)      \n",
    "sff  = df.iloc[:, SFF].astype(float)\n",
    "allX = pd.concat([bits, sff], axis=1)      \n",
    "\n",
    "# Calculating the correlation according to the column entry\n",
    "corr_bits = bits.corr(method=\"pearson\")     \n",
    "corr_sff  = sff.corr(method=\"spearman\")     \n",
    "corr_all  = allX.corr(method=\"pearson\")     \n",
    "\n",
    "# CLustering the higher correlations together for better visualisation \n",
    "def repair_corr(R: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Set diag=1 and replace off-diagonal NaNs with 0 so plots/clustering don’t break.\"\"\"\n",
    "    R = R.copy()\n",
    "    np.fill_diagonal(R.values, 1.0)\n",
    "    return R.fillna(0.0)\n",
    "\n",
    "def cluster_order(R: pd.DataFrame):\n",
    "    \"\"\"Average-linkage on distance = 1 - |r|. Returns index order.\"\"\"\n",
    "    R = repair_corr(R)\n",
    "    D = 1.0 - np.abs(R.values)\n",
    "    np.fill_diagonal(D, 0.0)\n",
    "    # condensed distance vector\n",
    "    dvec = squareform(D, checks=False)\n",
    "    if not np.isfinite(dvec).all():\n",
    "        raise ValueError(\"Non-finite values in distance; check for NaNs/inf in correlation.\")\n",
    "    Z = linkage(dvec, method=\"average\")\n",
    "    return leaves_list(Z)\n",
    "\n",
    "# Plotting the correlation matrix\n",
    "def plot_corr(R: pd.DataFrame, title: str, outfile: str = None,\n",
    "              annotate: bool = False, thresh: float | None = None,\n",
    "              figsize=(10, 8)):\n",
    "    R = repair_corr(R)\n",
    "    plt.figure(figsize=figsize)\n",
    "    im = plt.imshow(R.values, vmin=-1, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(im, label=\"correlation\")\n",
    "    plt.xticks(range(len(R.columns)), R.columns, rotation=90)\n",
    "    plt.yticks(range(len(R.index)),   R.index)\n",
    "\n",
    "    if annotate:\n",
    "        h, w = R.shape\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                val = R.iat[i, j]\n",
    "                if thresh is not None and abs(val) < thresh:\n",
    "                    continue\n",
    "                plt.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", fontsize=6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if outfile:\n",
    "        plt.savefig(outfile, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_corr_clustered(R: pd.DataFrame, title: str, outfile: str = None,\n",
    "                        annotate: bool = False, thresh: float | None = None,\n",
    "                        figsize=(10, 8)):\n",
    "    order = cluster_order(R)\n",
    "    R_ord = R.iloc[order, :].iloc[:, order]\n",
    "    plot_corr(R_ord, title + \" (clustered)\", outfile, annotate, thresh, figsize)\n",
    "\n",
    "# Plotting raw features only CM, SFF only CM and all together CM\n",
    "plot_corr(corr_bits, \"Bits-only correlation (phi)\", \"corr_bits_25x25.png\",\n",
    "          annotate=ANNOTATE, thresh=ANNOTATE_THRESH, figsize=FIGSIZE)\n",
    "plot_corr(corr_sff,  \"SFF-only correlation (Spearman)\", \"corr_sff_26x26.png\",\n",
    "          annotate=ANNOTATE, thresh=ANNOTATE_THRESH, figsize=FIGSIZE)\n",
    "plot_corr(corr_all,  \"All features correlation (Pearson)\", \"corr_all_51x51.png\",\n",
    "          annotate=True, figsize=FIGSIZE)\n",
    "\n",
    "# Clustered versions\n",
    "plot_corr_clustered(corr_bits, \"Bits correlation\", \"corr_bits_clustered.png\",\n",
    "                    annotate=False, figsize=FIGSIZE)\n",
    "plot_corr_clustered(corr_sff,  \"SFF correlation\",  \"corr_sff_clustered.png\",\n",
    "                    annotate=False, figsize=FIGSIZE)\n",
    "plot_corr_clustered(corr_all,  \"All features correlation\", \"corr_all_clustered.png\",\n",
    "                    annotate=False, figsize=FIGSIZE)\n",
    "\n",
    "# Saving matrices for inspection in Excel \n",
    "repair_corr(corr_bits).to_csv(\"corr_bits_25x25.csv\")\n",
    "repair_corr(corr_sff).to_csv(\"corr_sff_26x26.csv\")\n",
    "repair_corr(corr_all).to_csv(\"corr_all_51x51.csv\")\n",
    "print(\"Saved PNGs and CSVs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6607caa8-d4a8-42c1-a9fb-5d2448127c06",
   "metadata": {},
   "source": [
    "4) Removal of the fully correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5790834-009a-4289-a39f-d85c8a0f50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load your data\n",
    "df = pd.read_csv(\"sff_optimal_1.csv\")\n",
    "\n",
    "def drop_and_report(df, colname):\n",
    "    if colname in df.columns:\n",
    "        df = df.drop(columns=[colname])\n",
    "        print(f\"Removed column: {colname}\")\n",
    "    else:\n",
    "        print(f\"Column not found: {colname}\")\n",
    "    return df\n",
    "\n",
    "# removal of complete correlated features\n",
    "related_columns = [\"sff_C_2x3_up_open\", \"sff_C_3x2_right_open\", \"sff_L_2x2_down_right\", \"sff_L_2x2_down_left\", \"sff_L_2x2_up_left\", \"sff_T_3x3_left\", \"sff_T_3x3_up\"]\n",
    "for cName in related_columns:\n",
    "    df = drop_and_report(df, cName)\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29910cf2-7e27-4f49-8325-9906ad63c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_columns=[\"sff_cross_3x3\",\"sff_diag_main_3x3\",\"sff_diag_anti_3x3\",\"sff_checker_2x2\",\"sff_plus_3x3\",\"sff_T_3x3_right\",\"sff_T_3x3_down\"]\n",
    "for cName in rare_columns:\n",
    "    df = drop_and_report(df, cName)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7f2052-0721-4d83-8e02-4697003f7782",
   "metadata": {},
   "source": [
    "5) Safecheck and feature engineering for edited dataset (also useful for presentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a9e7b-4877-4158-a876-d7aac3b91446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Loading the slices of raw geometry bits and shape frequency features\n",
    "BITS_SLICE = slice(0, 25)      # 25 binary columns\n",
    "SFF_SLICE  = slice(25, 51)     # 26 SFF probability columns\n",
    "TARGET_COL = df.columns[-1]    # last column is bucket (0..3)\n",
    "\n",
    "# Preparing blocks\n",
    "bits = df.iloc[:, BITS_SLICE].astype(int)\n",
    "sff  = df.iloc[:, SFF_SLICE].astype(float)\n",
    "y    = df[TARGET_COL].astype(int).values\n",
    "\n",
    "# safety against constant columns (MI will be 0 for those)\n",
    "bits_var0 = bits.columns[(bits.var() == 0).values].tolist()\n",
    "sff_var0  = sff.columns[(sff.var() == 0).values].tolist()\n",
    "if bits_var0 or sff_var0:\n",
    "    print(\"Warning: constant columns (MI will be 0):\")\n",
    "    if bits_var0: print(\"  Bits:\", bits_var0)\n",
    "    if sff_var0:  print(\"  SFF :\", sff_var0)\n",
    "\n",
    "# Mutual Information\n",
    "mi_bits = mutual_info_classif(bits.values, y, discrete_features=True,  random_state=0)\n",
    "mi_sff  = mutual_info_classif(sff.values,  y, discrete_features=False, random_state=0)\n",
    "\n",
    "mi_bits_s = pd.Series(mi_bits, index=bits.columns, name=\"mi\")\n",
    "mi_sff_s  = pd.Series(mi_sff,  index=sff.columns,  name=\"mi\")\n",
    "\n",
    "mi_all = (\n",
    "    pd.concat([mi_bits_s, mi_sff_s])\n",
    "      .to_frame()\n",
    "      .assign(block=lambda d: np.where(d.index.isin(bits.columns), \"bits\", \"sff\"))\n",
    "      .sort_values(\"mi\", ascending=False)\n",
    "      .reset_index(names=\"feature\")\n",
    ")\n",
    "\n",
    "# Top-10 and Bottom-10\n",
    "top10    = mi_all.head(10).copy()\n",
    "bottom10 = mi_all.tail(10).sort_values(\"mi\", ascending=True).copy()\n",
    "\n",
    "print(\"\\nTop-10 most discriminative (by mutual information):\")\n",
    "print(top10.to_string(index=False))\n",
    "\n",
    "print(\"\\nBottom-10 least discriminative (by mutual information):\")\n",
    "print(bottom10.to_string(index=False))\n",
    "\n",
    "# bar plots\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.barh(top10[\"feature\"], top10[\"mi\"])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Top-10 MI features\")\n",
    "plt.xlabel(\"Mutual Information\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.barh(bottom10[\"feature\"], bottom10[\"mi\"])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Bottom-10 MI features\")\n",
    "plt.xlabel(\"Mutual Information\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e7448-f316-4a29-8f74-249dde280d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "\n",
    "# Loading the dataset\n",
    "\n",
    "CSV_PATH = \"sff_with_pca_optimal.csv\"\n",
    "\n",
    "bits = df.iloc[:, 0:25].astype(int)\n",
    "sff  = df.iloc[:, 25:25+26].astype(float)\n",
    "y    = df.iloc[:, -1].astype(int)\n",
    "n    = len(df)\n",
    "\n",
    "\n",
    "# Mutual Information (MI)\n",
    "\n",
    "mi_bits = mutual_info_classif(bits.values, y, discrete_features=True,  random_state=0)\n",
    "mi_sff  = mutual_info_classif(sff.values,  y, discrete_features=False, random_state=0)\n",
    "\n",
    "mi_bits_s = pd.Series(mi_bits, index=bits.columns, name=\"mi\")\n",
    "mi_sff_s  = pd.Series(mi_sff,  index=sff.columns,  name=\"mi\")\n",
    "\n",
    "# Top/bottom by MI \n",
    "mi_all = (\n",
    "    pd.concat([mi_bits_s, mi_sff_s])\n",
    "      .to_frame()\n",
    "      .assign(block=lambda d: np.where(d.index.isin(bits.columns), \"bits\", \"sff\"))\n",
    "      .sort_values(\"mi\", ascending=False)\n",
    "      .reset_index(names=\"feature\")\n",
    ")\n",
    "print(\"\\nTop-10 (ALL) by MI\")\n",
    "print(mi_all.head(10).to_string(index=False))\n",
    "print(\"\\nBottom-10 (ALL) by MI\")\n",
    "print(mi_all.tail(10).sort_values(\"mi\").to_string(index=False))\n",
    "\n",
    "\n",
    "# χ² + Cramér’s V (for bits only)\n",
    "classes_present = sorted(pd.unique(y))\n",
    "\n",
    "def cramers_v_safe(table_values: np.ndarray):\n",
    "    n_obs = table_values.sum()\n",
    "    if n_obs == 0:\n",
    "        return np.nan, np.nan\n",
    "    if (table_values.sum(axis=0) == 0).any() or (table_values.sum(axis=1) == 0).any():\n",
    "        return np.nan, np.nan\n",
    "    r, c = table_values.shape\n",
    "    if r < 2 or c < 2:\n",
    "        return np.nan, np.nan\n",
    "    chi2, p, dof, _ = chi2_contingency(table_values, correction=False)\n",
    "    denom = n_obs * (min(r - 1, c - 1))\n",
    "    if denom <= 0:\n",
    "        return np.nan, p\n",
    "    V = np.sqrt(chi2 / denom)\n",
    "    return V, p\n",
    "\n",
    "rows = []\n",
    "for col in bits.columns:\n",
    "    tab = pd.crosstab(bits[col], y)\n",
    "    tab = tab.reindex(index=[0, 1], columns=classes_present, fill_value=0)\n",
    "    V, p = cramers_v_safe(tab.values)\n",
    "    rows.append({\n",
    "        \"feature\": col,\n",
    "        \"cramers_v\": V,\n",
    "        \"p_value\": p,\n",
    "        \"count_ones\": int(bits[col].sum()),\n",
    "        \"count_zeros\": int((bits[col] == 0).sum())\n",
    "    })\n",
    "\n",
    "cv_bits = pd.DataFrame(rows)\n",
    "cv_top10    = cv_bits.sort_values(\"cramers_v\", ascending=False, na_position=\"last\").head(10)\n",
    "cv_bottom10 = cv_bits.sort_values(\"cramers_v\", ascending=True,  na_position=\"last\").head(10)\n",
    "\n",
    "print(\"\\nTop-10 bits by Cramér’s V\")\n",
    "print(cv_top10.to_string(index=False))\n",
    "print(\"\\nBottom-10 bits by Cramér’s V\")\n",
    "print(cv_bottom10.to_string(index=False))\n",
    "\n",
    "# ΔP(class | bit=1) - ΔP(class | bit=0)\n",
    "# for top-10 and bottom-10 MI bits\n",
    "def cond_probs_bit(col):\n",
    "    sub1 = df[bits[col] == 1][y.name].value_counts().reindex([0,1,2,3], fill_value=0)\n",
    "    sub0 = df[bits[col] == 0][y.name].value_counts().reindex([0,1,2,3], fill_value=0)\n",
    "    p1 = (sub1 / max(1, sub1.sum())).values\n",
    "    p0 = (sub0 / max(1, sub0.sum())).values\n",
    "    return p1, p0\n",
    "\n",
    "top_bits_mi    = mi_bits_s.sort_values(ascending=False).head(10).index\n",
    "bottom_bits_mi = mi_bits_s.sort_values(ascending=True).head(10).index\n",
    "\n",
    "def plot_deltaP(bit_list, title):\n",
    "    mat = []\n",
    "    for col in bit_list:\n",
    "        p1, p0 = cond_probs_bit(col)\n",
    "        mat.append(p1 - p0)\n",
    "    mat = np.vstack(mat)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(mat, vmin=-1, vmax=1)\n",
    "    plt.yticks(range(len(bit_list)), bit_list)\n",
    "    plt.xticks(range(4), [0,1,2,3])\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label=\"Probability shift\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_deltaP(top_bits_mi,    \"ΔP(class | bit=1) vs bit=0 — TOP-10 MI bits\")\n",
    "plot_deltaP(bottom_bits_mi, \"ΔP(class | bit=1) vs bit=0 — BOTTOM-10 MI bits\")\n",
    "\n",
    "\n",
    "# SFF class-conditional means:\n",
    "# TOP-10 & BOTTOM-10 by std across classes\n",
    "\n",
    "sff_means = sff.assign(cls=y).groupby(\"cls\").mean().T  # rows=SFFs, cols=classes\n",
    "std_across = sff_means.std(axis=1)\n",
    "\n",
    "top10_sff    = std_across.sort_values(ascending=False).head(10).index\n",
    "bottom10_sff = std_across.sort_values(ascending=True).head(10).index\n",
    "\n",
    "def plot_sff_means(rows, title):\n",
    "    vals = sff_means.loc[rows].values\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.imshow(vals, aspect='auto')\n",
    "    plt.yticks(range(len(rows)), rows)\n",
    "    plt.xticks(range(4), [0,1,2,3])\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label=\"E[SFF | class]\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_sff_means(top10_sff,    \"E[SFF | class] — TOP-10 discriminative SFFs (by std)\")\n",
    "plot_sff_means(bottom10_sff, \"E[SFF | class] — BOTTOM-10 least discriminative SFFs (by std)\")\n",
    "\n",
    "\n",
    "# Volume fraction \n",
    "vf = bits.sum(axis=1) / 25.0\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(vf, bins=20)\n",
    "plt.title(\"Volume fraction (overall)\")\n",
    "plt.xlabel(\"VF\"); plt.ylabel(\"count\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for c in [0,1,2,3]:\n",
    "    plt.hist(vf[y == c], bins=20, alpha=0.6, label=str(c))\n",
    "plt.title(\"Volume fraction by class\")\n",
    "plt.xlabel(\"VF\"); plt.ylabel(\"count\")\n",
    "plt.legend(title=\"class\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c3aad-6aea-446b-ba00-c92744f20dfb",
   "metadata": {},
   "source": [
    "6) Dropped more features as per the importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf687717-fd46-49fb-adcd-cb1966add21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MI0_columns = [\"sff_C_2x3_down_open\", \"sff_C_3x2_left_open\"]\n",
    "for cName in MI0_columns:\n",
    "    df = drop_and_report(df, cName)\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e855b8be-051b-459e-b96d-d782ce1b28b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_path = \"feature_engineered_data.csv\"\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"Saved to {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
