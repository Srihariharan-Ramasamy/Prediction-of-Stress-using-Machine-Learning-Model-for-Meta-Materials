{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "GUhLBEbBDVjO",
      "metadata": {
        "id": "GUhLBEbBDVjO"
      },
      "source": [
        "Analysis of the Raw+SFF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1695e85-2f93-44db-b5b8-e98506e68ffa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1695e85-2f93-44db-b5b8-e98506e68ffa",
        "outputId": "30403ea3-a83d-415e-b777-1460fedaf42e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, classification_report, confusion_matrix,\n",
        "    ConfusionMatrixDisplay, log_loss, top_k_accuracy_score, roc_auc_score\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# 1) Loading the data\n",
        "file_path = \"/content/drive/MyDrive/feature_engineered_with_pca_optimal.csv\"\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.head())\n",
        "df = df.loc[:, ~df.columns.astype(str).str.startswith(\"Unnamed\")]\n",
        "\n",
        "\n",
        "X_df = df.iloc[:, 0:35]\n",
        "y = df.iloc[:,35]\n",
        "\n",
        "# 2) Train/test splitting\n",
        "X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
        "    X_df, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a19522e-78e2-4305-a131-703ce61da98e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a19522e-78e2-4305-a131-703ce61da98e",
        "outputId": "e089576b-f17a-4e63-c7b9-d8caafce8106"
      },
      "outputs": [],
      "source": [
        "%pip install -U xgboost\n",
        "import xgboost, sys\n",
        "print(\"xgboost\", xgboost.__version__, \"in\", sys.executable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ea35629-30e2-4b46-8993-1c121b7ab1d0",
      "metadata": {
        "id": "0ea35629-30e2-4b46-8993-1c121b7ab1d0"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84071fc5-374d-4483-afda-28d03615b8f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84071fc5-374d-4483-afda-28d03615b8f6",
        "outputId": "a2d6ae48-acf2-4c9f-c034-9a778f88c4de"
      },
      "outputs": [],
      "source": [
        "# RandomizedSearchCV for XGBoost\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.metrics import log_loss, accuracy_score, f1_score\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.stats import randint, uniform, loguniform\n",
        "\n",
        "# model\n",
        "xgb_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"xgb\", XGBClassifier(\n",
        "        objective=\"multi:softprob\",\n",
        "        num_class=4,\n",
        "        tree_method=\"hist\",\n",
        "        eval_metric=\"mlogloss\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Cross Validation and scoring\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "scoring = {\n",
        "    \"neg_log_loss\": \"neg_log_loss\",\n",
        "    \"accuracy\": \"accuracy\",\n",
        "    \"f1_macro\": \"f1_macro\",\n",
        "}\n",
        "\n",
        "# Parameters\n",
        "param_dist = {\n",
        "    \"xgb__n_estimators\": randint(100, 1001),\n",
        "    \"xgb__learning_rate\": loguniform(1e-3, 3e-1),\n",
        "    \"xgb__max_depth\": randint(3, 9),\n",
        "    \"xgb__min_child_weight\": randint(1, 9),\n",
        "    \"xgb__subsample\": uniform(0.6, 0.4),\n",
        "    \"xgb__colsample_bytree\": uniform(0.6, 0.4),\n",
        "    \"xgb__gamma\": loguniform(1e-8, 1e1),\n",
        "    \"xgb__reg_alpha\": loguniform(1e-8, 1e1),\n",
        "    \"xgb__reg_lambda\": loguniform(1e-2, 1e2),\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=xgb_pipe,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,\n",
        "    cv=cv,\n",
        "    scoring=scoring,\n",
        "    refit=\"neg_log_loss\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# searching\n",
        "search.fit(X_train_df, y_train)\n",
        "\n",
        "# results table\n",
        "cvres = pd.DataFrame(search.cv_results_)\n",
        "results = (\n",
        "    cvres.assign(\n",
        "        train_logloss = -cvres[\"mean_train_neg_log_loss\"],\n",
        "        cv_logloss    = -cvres[\"mean_test_neg_log_loss\"],\n",
        "        train_acc     =  cvres[\"mean_train_accuracy\"],\n",
        "        cv_acc        =  cvres[\"mean_test_accuracy\"],\n",
        "        train_f1_macro=  cvres[\"mean_train_f1_macro\"],\n",
        "        cv_f1_macro   =  cvres[\"mean_test_f1_macro\"],\n",
        "    )[\n",
        "        [\"param_xgb__n_estimators\",\"param_xgb__learning_rate\",\"param_xgb__max_depth\",\n",
        "         \"param_xgb__min_child_weight\",\"param_xgb__subsample\",\"param_xgb__colsample_bytree\",\n",
        "         \"param_xgb__gamma\",\"param_xgb__reg_alpha\",\"param_xgb__reg_lambda\",\n",
        "         \"train_logloss\",\"cv_logloss\",\"train_acc\",\"cv_acc\",\"train_f1_macro\",\"cv_f1_macro\"]\n",
        "    ]\n",
        "    .rename(columns={\n",
        "        \"param_xgb__n_estimators\":\"n_estimators\",\n",
        "        \"param_xgb__learning_rate\":\"learning_rate\",\n",
        "        \"param_xgb__max_depth\":\"max_depth\",\n",
        "        \"param_xgb__min_child_weight\":\"min_child_weight\",\n",
        "        \"param_xgb__subsample\":\"subsample\",\n",
        "        \"param_xgb__colsample_bytree\":\"colsample_bytree\",\n",
        "        \"param_xgb__gamma\":\"gamma\",\n",
        "        \"param_xgb__reg_alpha\":\"reg_alpha\",\n",
        "        \"param_xgb__reg_lambda\":\"reg_lambda\",\n",
        "    })\n",
        "    .sort_values(\"cv_logloss\")\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "display(results.head(20))\n",
        "\n",
        "print(\"\\nBest params (by CV log-loss):\")\n",
        "print(search.best_params_)\n",
        "print(f\"Best CV log-loss: {-search.best_score_:.5f}\")\n",
        "\n",
        "# ----- quick visuals (optional)\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.scatter(results[\"n_estimators\"], results[\"cv_logloss\"])\n",
        "plt.xlabel(\"n_estimators\"); plt.ylabel(\"CV Log-loss (lower is better)\")\n",
        "plt.title(\"XGB RandomizedSearch — CV Log-loss vs n_estimators\")\n",
        "plt.grid(True); plt.show()\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.scatter(results[\"learning_rate\"].astype(float), results[\"cv_f1_macro\"])\n",
        "plt.xlabel(\"learning_rate\"); plt.ylabel(\"CV F1 (macro)\")\n",
        "plt.title(\"XGB RandomizedSearch — CV F1 (macro) vs learning_rate\")\n",
        "plt.grid(True); plt.show()\n",
        "\n",
        "# Final model\n",
        "final_model = search.best_estimator_\n",
        "final_model.fit(X_train_df, y_train)\n",
        "\n",
        "proba_test = final_model.predict_proba(X_test_df)\n",
        "pred_test  = final_model.predict(X_test_df)\n",
        "\n",
        "test_logloss  = log_loss(y_test, proba_test, labels=np.unique(y_train))\n",
        "test_acc      = accuracy_score(y_test, pred_test)\n",
        "test_f1_macro = f1_score(y_test, pred_test, average=\"macro\")\n",
        "\n",
        "print(f\"\\nTest log-loss: {test_logloss:.4f}\")\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test F1 (macro): {test_f1_macro:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "314fbc1a-3f36-4ef2-971d-95441c945242",
      "metadata": {
        "id": "314fbc1a-3f36-4ef2-971d-95441c945242"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# best params from the search above\n",
        "best_params = {\n",
        "    \"xgb__colsample_bytree\": float(0.7334834444556088),\n",
        "    \"xgb__gamma\":            float(1.931084870540406e-07),\n",
        "    \"xgb__learning_rate\":    float(0.040957144541603416),\n",
        "    \"xgb__max_depth\":        7,\n",
        "    \"xgb__min_child_weight\": 2,\n",
        "    \"xgb__n_estimators\":     443,\n",
        "    \"xgb__reg_alpha\":        float(0.31044435499483225),\n",
        "    \"xgb__reg_lambda\":       float(0.07068974950624607),\n",
        "    \"xgb__subsample\":        float(0.6727299868828402),\n",
        "}\n",
        "\n",
        "# pipeline\n",
        "xgb_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"xgb\", XGBClassifier(\n",
        "        objective=\"multi:softprob\",\n",
        "        num_class=4,              # <-- set to your number of classes\n",
        "        tree_method=\"hist\",\n",
        "        eval_metric=\"mlogloss\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# applying the best params to the pipeline\n",
        "xgb_pipe.set_params(**best_params)\n",
        "\n",
        "# fittin on training data\n",
        "xgb_pipe.fit(X_train_df, y_train)\n",
        "\n",
        "# predicting\n",
        "y_pred = xgb_pipe.predict(X_test_df)\n",
        "\n",
        "# confusion matrix\n",
        "labels = np.unique(y_train)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"true_{l}\" for l in labels], columns=[f\"pred_{l}\" for l in labels])\n",
        "display(cm_df)\n",
        "\n",
        "# plot\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "disp.plot(ax=ax, cmap=\"Blues\", values_format=\"d\", colorbar=False)\n",
        "plt.title(\"XGBoost Confusion Matrix (best params)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "832b7960-cc8b-4d39-82bc-d3f24272f502",
      "metadata": {
        "id": "832b7960-cc8b-4d39-82bc-d3f24272f502"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, classification_report\n",
        "\n",
        "# F1 scores\n",
        "f1_macro    = f1_score(y_test, y_pred, average=\"macro\")\n",
        "f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "print(f\"F1 (macro):    {f1_macro:.4f}\")\n",
        "print(f\"F1 (weighted): {f1_weighted:.4f}\")\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28070e1b-e83f-4c39-99be-470835e98444",
      "metadata": {
        "id": "28070e1b-e83f-4c39-99be-470835e98444"
      },
      "outputs": [],
      "source": [
        "\n",
        "feat_names = list(X_train_df.columns)\n",
        "\n",
        "# SHAP global summary for interpretability\n",
        "try:\n",
        "    import shap\n",
        "\n",
        "    X_test_imp = pd.DataFrame(\n",
        "        xgb_pipe.named_steps[\"imp\"].transform(X_test_df),\n",
        "        columns=feat_names, index=X_test_df.index\n",
        "    )\n",
        "    explainer = shap.TreeExplainer(xgb_pipe.named_steps[\"xgb\"])\n",
        "    sample_n = min(400, len(X_test_imp))\n",
        "    X_shap = X_test_imp.sample(sample_n, random_state=42)\n",
        "    shap_values = explainer(X_shap, check_additivity=False)\n",
        "\n",
        "    shap.summary_plot(shap_values, X_shap, show=True)\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\nSHAP skipped (optional). Reason:\", repr(e))\n",
        "    print(\"Tip: `pip install shap` to enable SHAP plots.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bAeJP3spD-Fr",
      "metadata": {
        "id": "bAeJP3spD-Fr"
      },
      "source": [
        "Analysis of Raw features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "guT02ntuiWAo",
      "metadata": {
        "id": "guT02ntuiWAo"
      },
      "outputs": [],
      "source": [
        "X_df = df.iloc[:, 0:25]\n",
        "y = df.iloc[:,35]\n",
        "\n",
        "# 2) Train/test splitting\n",
        "X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
        "    X_df, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# model\n",
        "xgb_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"xgb\", XGBClassifier(\n",
        "        objective=\"multi:softprob\",\n",
        "        num_class=4,\n",
        "        tree_method=\"hist\",\n",
        "        eval_metric=\"mlogloss\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Cross Validation and scoring\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "scoring = {\n",
        "    \"neg_log_loss\": \"neg_log_loss\",\n",
        "    \"accuracy\": \"accuracy\",\n",
        "    \"f1_macro\": \"f1_macro\",\n",
        "}\n",
        "\n",
        "# Parameters\n",
        "param_dist = {\n",
        "    \"xgb__n_estimators\": randint(100, 1001),\n",
        "    \"xgb__learning_rate\": loguniform(1e-3, 3e-1),\n",
        "    \"xgb__max_depth\": randint(3, 9),\n",
        "    \"xgb__min_child_weight\": randint(1, 9),\n",
        "    \"xgb__subsample\": uniform(0.6, 0.4),\n",
        "    \"xgb__colsample_bytree\": uniform(0.6, 0.4),\n",
        "    \"xgb__gamma\": loguniform(1e-8, 1e1),\n",
        "    \"xgb__reg_alpha\": loguniform(1e-8, 1e1),\n",
        "    \"xgb__reg_lambda\": loguniform(1e-2, 1e2),\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=xgb_pipe,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,\n",
        "    cv=cv,\n",
        "    scoring=scoring,\n",
        "    refit=\"neg_log_loss\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# searching\n",
        "search.fit(X_train_df, y_train)\n",
        "\n",
        "# results table\n",
        "cvres = pd.DataFrame(search.cv_results_)\n",
        "results = (\n",
        "    cvres.assign(\n",
        "        train_logloss = -cvres[\"mean_train_neg_log_loss\"],\n",
        "        cv_logloss    = -cvres[\"mean_test_neg_log_loss\"],\n",
        "        train_acc     =  cvres[\"mean_train_accuracy\"],\n",
        "        cv_acc        =  cvres[\"mean_test_accuracy\"],\n",
        "        train_f1_macro=  cvres[\"mean_train_f1_macro\"],\n",
        "        cv_f1_macro   =  cvres[\"mean_test_f1_macro\"],\n",
        "    )[\n",
        "        [\"param_xgb__n_estimators\",\"param_xgb__learning_rate\",\"param_xgb__max_depth\",\n",
        "         \"param_xgb__min_child_weight\",\"param_xgb__subsample\",\"param_xgb__colsample_bytree\",\n",
        "         \"param_xgb__gamma\",\"param_xgb__reg_alpha\",\"param_xgb__reg_lambda\",\n",
        "         \"train_logloss\",\"cv_logloss\",\"train_acc\",\"cv_acc\",\"train_f1_macro\",\"cv_f1_macro\"]\n",
        "    ]\n",
        "    .rename(columns={\n",
        "        \"param_xgb__n_estimators\":\"n_estimators\",\n",
        "        \"param_xgb__learning_rate\":\"learning_rate\",\n",
        "        \"param_xgb__max_depth\":\"max_depth\",\n",
        "        \"param_xgb__min_child_weight\":\"min_child_weight\",\n",
        "        \"param_xgb__subsample\":\"subsample\",\n",
        "        \"param_xgb__colsample_bytree\":\"colsample_bytree\",\n",
        "        \"param_xgb__gamma\":\"gamma\",\n",
        "        \"param_xgb__reg_alpha\":\"reg_alpha\",\n",
        "        \"param_xgb__reg_lambda\":\"reg_lambda\",\n",
        "    })\n",
        "    .sort_values(\"cv_logloss\")\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "display(results.head(20))\n",
        "\n",
        "print(\"\\nBest params (by CV log-loss):\")\n",
        "print(search.best_params_)\n",
        "print(f\"Best CV log-loss: {-search.best_score_:.5f}\")\n",
        "\n",
        "# ----- quick visuals (optional)\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.scatter(results[\"n_estimators\"], results[\"cv_logloss\"])\n",
        "plt.xlabel(\"n_estimators\"); plt.ylabel(\"CV Log-loss (lower is better)\")\n",
        "plt.title(\"XGB RandomizedSearch — CV Log-loss vs n_estimators\")\n",
        "plt.grid(True); plt.show()\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.scatter(results[\"learning_rate\"].astype(float), results[\"cv_f1_macro\"])\n",
        "plt.xlabel(\"learning_rate\"); plt.ylabel(\"CV F1 (macro)\")\n",
        "plt.title(\"XGB RandomizedSearch — CV F1 (macro) vs learning_rate\")\n",
        "plt.grid(True); plt.show()\n",
        "\n",
        "# Final model\n",
        "final_model = search.best_estimator_\n",
        "final_model.fit(X_train_df, y_train)\n",
        "\n",
        "proba_test = final_model.predict_proba(X_test_df)\n",
        "pred_test  = final_model.predict(X_test_df)\n",
        "\n",
        "test_logloss  = log_loss(y_test, proba_test, labels=np.unique(y_train))\n",
        "test_acc      = accuracy_score(y_test, pred_test)\n",
        "test_f1_macro = f1_score(y_test, pred_test, average=\"macro\")\n",
        "\n",
        "print(f\"\\nTest log-loss: {test_logloss:.4f}\")\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test F1 (macro): {test_f1_macro:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# best params from the search above\n",
        "best_params = {\n",
        "    \"xgb__colsample_bytree\": float(0.7334834444556088),\n",
        "    \"xgb__gamma\":            float(1.931084870540406e-07),\n",
        "    \"xgb__learning_rate\":    float(0.040957144541603416),\n",
        "    \"xgb__max_depth\":        7,\n",
        "    \"xgb__min_child_weight\": 2,\n",
        "    \"xgb__n_estimators\":     443,\n",
        "    \"xgb__reg_alpha\":        float(0.31044435499483225),\n",
        "    \"xgb__reg_lambda\":       float(0.07068974950624607),\n",
        "    \"xgb__subsample\":        float(0.6727299868828402),\n",
        "}\n",
        "\n",
        "# pipeline\n",
        "xgb_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"xgb\", XGBClassifier(\n",
        "        objective=\"multi:softprob\",\n",
        "        num_class=4,              # <-- set to your number of classes\n",
        "        tree_method=\"hist\",\n",
        "        eval_metric=\"mlogloss\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# applying the best params to the pipeline\n",
        "xgb_pipe.set_params(**best_params)\n",
        "\n",
        "# fittin on training data\n",
        "xgb_pipe.fit(X_train_df, y_train)\n",
        "\n",
        "# predicting\n",
        "y_pred = xgb_pipe.predict(X_test_df)\n",
        "\n",
        "# confusion matrix\n",
        "labels = np.unique(y_train)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"true_{l}\" for l in labels], columns=[f\"pred_{l}\" for l in labels])\n",
        "display(cm_df)\n",
        "\n",
        "# plot\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "disp.plot(ax=ax, cmap=\"Blues\", values_format=\"d\", colorbar=False)\n",
        "plt.title(\"XGBoost Confusion Matrix (best params)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OpbMhXn3E1My",
      "metadata": {
        "id": "OpbMhXn3E1My"
      },
      "source": [
        "Analysis of PCA features (17 features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fVSUPC4kl402",
      "metadata": {
        "id": "fVSUPC4kl402"
      },
      "outputs": [],
      "source": [
        "X_df = df.iloc[:, 36:53]\n",
        "y = df.iloc[:,35]\n",
        "\n",
        "# 2) Train/test splitting\n",
        "X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
        "    X_df, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# model\n",
        "xgb_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"xgb\", XGBClassifier(\n",
        "        objective=\"multi:softprob\",\n",
        "        num_class=4,\n",
        "        tree_method=\"hist\",\n",
        "        eval_metric=\"mlogloss\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Cross Validation and scoring\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "scoring = {\n",
        "    \"neg_log_loss\": \"neg_log_loss\",\n",
        "    \"accuracy\": \"accuracy\",\n",
        "    \"f1_macro\": \"f1_macro\",\n",
        "}\n",
        "\n",
        "# Parameters\n",
        "param_dist = {\n",
        "    \"xgb__n_estimators\": randint(100, 1001),\n",
        "    \"xgb__learning_rate\": loguniform(1e-3, 3e-1),\n",
        "    \"xgb__max_depth\": randint(3, 9),\n",
        "    \"xgb__min_child_weight\": randint(1, 9),\n",
        "    \"xgb__subsample\": uniform(0.6, 0.4),\n",
        "    \"xgb__colsample_bytree\": uniform(0.6, 0.4),\n",
        "    \"xgb__gamma\": loguniform(1e-8, 1e1),\n",
        "    \"xgb__reg_alpha\": loguniform(1e-8, 1e1),\n",
        "    \"xgb__reg_lambda\": loguniform(1e-2, 1e2),\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=xgb_pipe,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,\n",
        "    cv=cv,\n",
        "    scoring=scoring,\n",
        "    refit=\"neg_log_loss\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# searching\n",
        "search.fit(X_train_df, y_train)\n",
        "\n",
        "# results table\n",
        "cvres = pd.DataFrame(search.cv_results_)\n",
        "results = (\n",
        "    cvres.assign(\n",
        "        train_logloss = -cvres[\"mean_train_neg_log_loss\"],\n",
        "        cv_logloss    = -cvres[\"mean_test_neg_log_loss\"],\n",
        "        train_acc     =  cvres[\"mean_train_accuracy\"],\n",
        "        cv_acc        =  cvres[\"mean_test_accuracy\"],\n",
        "        train_f1_macro=  cvres[\"mean_train_f1_macro\"],\n",
        "        cv_f1_macro   =  cvres[\"mean_test_f1_macro\"],\n",
        "    )[\n",
        "        [\"param_xgb__n_estimators\",\"param_xgb__learning_rate\",\"param_xgb__max_depth\",\n",
        "         \"param_xgb__min_child_weight\",\"param_xgb__subsample\",\"param_xgb__colsample_bytree\",\n",
        "         \"param_xgb__gamma\",\"param_xgb__reg_alpha\",\"param_xgb__reg_lambda\",\n",
        "         \"train_logloss\",\"cv_logloss\",\"train_acc\",\"cv_acc\",\"train_f1_macro\",\"cv_f1_macro\"]\n",
        "    ]\n",
        "    .rename(columns={\n",
        "        \"param_xgb__n_estimators\":\"n_estimators\",\n",
        "        \"param_xgb__learning_rate\":\"learning_rate\",\n",
        "        \"param_xgb__max_depth\":\"max_depth\",\n",
        "        \"param_xgb__min_child_weight\":\"min_child_weight\",\n",
        "        \"param_xgb__subsample\":\"subsample\",\n",
        "        \"param_xgb__colsample_bytree\":\"colsample_bytree\",\n",
        "        \"param_xgb__gamma\":\"gamma\",\n",
        "        \"param_xgb__reg_alpha\":\"reg_alpha\",\n",
        "        \"param_xgb__reg_lambda\":\"reg_lambda\",\n",
        "    })\n",
        "    .sort_values(\"cv_logloss\")\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "display(results.head(20))\n",
        "\n",
        "print(\"\\nBest params (by CV log-loss):\")\n",
        "print(search.best_params_)\n",
        "print(f\"Best CV log-loss: {-search.best_score_:.5f}\")\n",
        "\n",
        "# ----- quick visuals (optional)\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.scatter(results[\"n_estimators\"], results[\"cv_logloss\"])\n",
        "plt.xlabel(\"n_estimators\"); plt.ylabel(\"CV Log-loss (lower is better)\")\n",
        "plt.title(\"XGB RandomizedSearch — CV Log-loss vs n_estimators\")\n",
        "plt.grid(True); plt.show()\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.scatter(results[\"learning_rate\"].astype(float), results[\"cv_f1_macro\"])\n",
        "plt.xlabel(\"learning_rate\"); plt.ylabel(\"CV F1 (macro)\")\n",
        "plt.title(\"XGB RandomizedSearch — CV F1 (macro) vs learning_rate\")\n",
        "plt.grid(True); plt.show()\n",
        "\n",
        "# Final model\n",
        "final_model = search.best_estimator_\n",
        "final_model.fit(X_train_df, y_train)\n",
        "\n",
        "proba_test = final_model.predict_proba(X_test_df)\n",
        "pred_test  = final_model.predict(X_test_df)\n",
        "\n",
        "test_logloss  = log_loss(y_test, proba_test, labels=np.unique(y_train))\n",
        "test_acc      = accuracy_score(y_test, pred_test)\n",
        "test_f1_macro = f1_score(y_test, pred_test, average=\"macro\")\n",
        "\n",
        "print(f\"\\nTest log-loss: {test_logloss:.4f}\")\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test F1 (macro): {test_f1_macro:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# best params from the search above\n",
        "best_params = {\n",
        "    \"xgb__colsample_bytree\": float(0.7334834444556088),\n",
        "    \"xgb__gamma\":            float(1.931084870540406e-07),\n",
        "    \"xgb__learning_rate\":    float(0.040957144541603416),\n",
        "    \"xgb__max_depth\":        7,\n",
        "    \"xgb__min_child_weight\": 2,\n",
        "    \"xgb__n_estimators\":     443,\n",
        "    \"xgb__reg_alpha\":        float(0.31044435499483225),\n",
        "    \"xgb__reg_lambda\":       float(0.07068974950624607),\n",
        "    \"xgb__subsample\":        float(0.6727299868828402),\n",
        "}\n",
        "\n",
        "# pipeline\n",
        "xgb_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"xgb\", XGBClassifier(\n",
        "        objective=\"multi:softprob\",\n",
        "        num_class=4,              # <-- set to your number of classes\n",
        "        tree_method=\"hist\",\n",
        "        eval_metric=\"mlogloss\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# applying the best params to the pipeline\n",
        "xgb_pipe.set_params(**best_params)\n",
        "\n",
        "# fittin on training data\n",
        "xgb_pipe.fit(X_train_df, y_train)\n",
        "\n",
        "# predicting\n",
        "y_pred = xgb_pipe.predict(X_test_df)\n",
        "\n",
        "# confusion matrix\n",
        "labels = np.unique(y_train)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"true_{l}\" for l in labels], columns=[f\"pred_{l}\" for l in labels])\n",
        "display(cm_df)\n",
        "\n",
        "# plot\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "disp.plot(ax=ax, cmap=\"Blues\", values_format=\"d\", colorbar=False)\n",
        "plt.title(\"XGBoost Confusion Matrix (best params)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
